{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5df4sU5Lk34",
        "outputId": "4f4aa896-ec38-4743-843e-5486d9a5ba6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MovieLens 100k...\n",
            "Extracted to data\n",
            "Loaded ratings: (100000, 4)\n",
            "Train size: 99057 Test size: 943\n",
            "n_users, n_items: 943 1682\n",
            "Computing item-item cosine similarity...\n",
            "Computing user-user cosine similarity...\n",
            "RMSE Item-based CF: 1.0038\n",
            "RMSE User-based CF: 1.0053\n",
            "Item-CF Precision@10: 0.0015 Recall@10: 0.0148 (on 943 users)\n",
            "Epoch 1/15 RMSE(train)=1.0396\n",
            "Epoch 2/15 RMSE(train)=0.9739\n",
            "Epoch 3/15 RMSE(train)=0.9521\n",
            "Epoch 4/15 RMSE(train)=0.9397\n",
            "Epoch 5/15 RMSE(train)=0.9310\n",
            "Epoch 6/15 RMSE(train)=0.9242\n",
            "Epoch 7/15 RMSE(train)=0.9184\n",
            "Epoch 8/15 RMSE(train)=0.9130\n",
            "Epoch 9/15 RMSE(train)=0.9078\n",
            "Epoch 10/15 RMSE(train)=0.9025\n",
            "Epoch 11/15 RMSE(train)=0.8969\n",
            "Epoch 12/15 RMSE(train)=0.8907\n",
            "Epoch 13/15 RMSE(train)=0.8839\n",
            "Epoch 14/15 RMSE(train)=0.8765\n",
            "Epoch 15/15 RMSE(train)=0.8685\n",
            "RMSE Matrix Factorization (FunkSVD): 0.9882\n",
            "MF Precision@10: 0.0030 Recall@10: 0.0297\n",
            "Top-10 recommendations (item_id,score):\n",
            "169 4.744\n",
            "100 4.687\n",
            "50 4.674\n",
            "178 4.671\n",
            "357 4.623\n",
            "408 4.564\n",
            "64 4.560\n",
            "190 4.544\n",
            "483 4.537\n",
            "114 4.520\n"
          ]
        }
      ],
      "source": [
        "# recommender_non_dl.py\n",
        "# Requirements:\n",
        "# pip install numpy pandas scipy scikit-learn requests tqdm\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# ------------------------------\n",
        "# Step 1: Download & Load MovieLens 100k\n",
        "# ------------------------------\n",
        "def download_movielens_100k(dest_folder='data'):\n",
        "    url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "    os.makedirs(dest_folder, exist_ok=True)\n",
        "    zip_path = os.path.join(dest_folder, 'ml-100k.zip')\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(\"Downloading MovieLens 100k...\")\n",
        "        r = requests.get(url, stream=True, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(dest_folder)\n",
        "    print(\"Extracted to\", dest_folder)\n",
        "\n",
        "def load_movielens_100k(path='data/ml-100k/u.data'):\n",
        "    # u.data format: user_id \\t item_id \\t rating \\t timestamp\n",
        "    names = ['user_id','item_id','rating','timestamp']\n",
        "    df = pd.read_csv(path, sep='\\t', names=names, engine='python')\n",
        "    return df\n",
        "\n",
        "# ------------------------------\n",
        "# Step 2: Preprocess & train/test split (leave-one-out)\n",
        "# ------------------------------\n",
        "def leave_one_out_split(ratings_df, seed=42, min_ratings=2):\n",
        "    \"\"\"\n",
        "    For each user with >= min_ratings, hold out one random rating for test.\n",
        "    Returns train_df, test_df (both pandas DataFrames).\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    train_list = []\n",
        "    test_list = []\n",
        "\n",
        "    for user, group in ratings_df.groupby('user_id'):\n",
        "        if len(group) < min_ratings:\n",
        "            # all to train if not enough ratings\n",
        "            train_list.append(group)\n",
        "            continue\n",
        "        test_idx = np.random.choice(group.index, size=1, replace=False)\n",
        "        test_list.append(group.loc[test_idx])\n",
        "        train_list.append(group.drop(test_idx))\n",
        "\n",
        "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
        "    test_df = pd.concat(test_list).reset_index(drop=True) if test_list else pd.DataFrame(columns=ratings_df.columns)\n",
        "    return train_df, test_df\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: Utility - ID mapping & sparse matrix\n",
        "# ------------------------------\n",
        "def create_mappings(ratings_df):\n",
        "    unique_users = ratings_df['user_id'].unique()\n",
        "    unique_items = ratings_df['item_id'].unique()\n",
        "    user_to_idx = {u: i for i, u in enumerate(sorted(unique_users))}\n",
        "    item_to_idx = {i: j for j, i in enumerate(sorted(unique_items))}\n",
        "    idx_to_user = {i: u for u, i in user_to_idx.items()}\n",
        "    idx_to_item = {j: i for i, j in item_to_idx.items()}\n",
        "    return user_to_idx, item_to_idx, idx_to_user, idx_to_item\n",
        "\n",
        "def build_user_item_matrix(ratings_df, user_to_idx, item_to_idx, shape=None):\n",
        "    rows = ratings_df['user_id'].map(user_to_idx)\n",
        "    cols = ratings_df['item_id'].map(item_to_idx)\n",
        "    data = ratings_df['rating'].astype(float)\n",
        "    if shape is None:\n",
        "        shape = (len(user_to_idx), len(item_to_idx))\n",
        "    mat = csr_matrix((data, (rows, cols)), shape=shape)\n",
        "    return mat\n",
        "\n",
        "# ------------------------------\n",
        "# Step 4: User-User & Item-Item Collaborative Filtering\n",
        "# ------------------------------\n",
        "def compute_user_similarity(user_item_csr):\n",
        "    # returns dense similarity matrix (users x users)\n",
        "    # for modest datasets (<=10k users) this is OK; for larger, use approximate methods\n",
        "    print(\"Computing user-user cosine similarity...\")\n",
        "    # convert to dense or use cosine_similarity on sparse (works)\n",
        "    sim = cosine_similarity(user_item_csr, dense_output=True)\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "    return sim\n",
        "\n",
        "def compute_item_similarity(user_item_csr):\n",
        "    # item vectors are columns, so take transpose\n",
        "    print(\"Computing item-item cosine similarity...\")\n",
        "    sim = cosine_similarity(user_item_csr.T, dense_output=True)\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "    return sim\n",
        "\n",
        "def predict_user_based(user_idx, item_idx, user_item_csr, user_sim, k=20, user_means=None):\n",
        "    # Weighted sum of k most similar users who rated the item\n",
        "    if user_means is None:\n",
        "        user_means = np.zeros(user_item_csr.shape[0])\n",
        "    sims = user_sim[user_idx]\n",
        "    # find users who rated the item\n",
        "    item_col = user_item_csr[:, item_idx].toarray().ravel()\n",
        "    rated_mask = item_col > 0\n",
        "    if not rated_mask.any():\n",
        "        return user_means[user_idx]  # fallback to user mean\n",
        "    # sort candidate users by similarity\n",
        "    candidate_idxs = np.where(rated_mask)[0]\n",
        "    candidate_sims = sims[candidate_idxs]\n",
        "    top_k_idx = candidate_idxs[np.argsort(candidate_sims)[-k:]][::-1]\n",
        "    top_sims = sims[top_k_idx]\n",
        "    top_ratings = item_col[top_k_idx]\n",
        "    # mean-centering (optional)\n",
        "    numer = ((top_ratings - user_means[top_k_idx]) * top_sims).sum()\n",
        "    denom = np.abs(top_sims).sum()\n",
        "    if denom == 0:\n",
        "        return user_means[user_idx]\n",
        "    pred = user_means[user_idx] + numer / denom\n",
        "    # clip\n",
        "    return min(5.0, max(1.0, pred))\n",
        "\n",
        "def predict_item_based(user_idx, item_idx, user_item_csr, item_sim, k=20):\n",
        "    # Use items user has rated and item-item similarity to target item\n",
        "    user_row = user_item_csr[user_idx].toarray().ravel()\n",
        "    rated_mask = user_row > 0\n",
        "    if not rated_mask.any():\n",
        "        return user_row.mean() if user_row.size > 0 else 3.0\n",
        "    candidate_item_idxs = np.where(rated_mask)[0]\n",
        "    sims = item_sim[item_idx, candidate_item_idxs]\n",
        "    top_k_idx = candidate_item_idxs[np.argsort(sims)[-k:]][::-1]\n",
        "    top_sims = sims[np.argsort(sims)[-k:]][::-1]\n",
        "    top_ratings = user_row[top_k_idx]\n",
        "    denom = np.abs(top_sims).sum()\n",
        "    if denom == 0:\n",
        "        return user_row.mean()\n",
        "    pred = (top_sims * top_ratings).sum() / denom\n",
        "    return min(5.0, max(1.0, pred))\n",
        "\n",
        "# ------------------------------\n",
        "# Step 5: Matrix Factorization (FunkSVD) via SGD (with biases)\n",
        "# ------------------------------\n",
        "class FunkSVD:\n",
        "    def __init__(self, n_factors=20, lr=0.01, reg=0.02, n_epochs=20, verbose=True, seed=42):\n",
        "        self.n_factors = n_factors\n",
        "        self.lr = lr\n",
        "        self.reg = reg\n",
        "        self.n_epochs = n_epochs\n",
        "        self.verbose = verbose\n",
        "        self.seed = seed\n",
        "\n",
        "    def fit(self, train_df, n_users, n_items, user_to_idx, item_to_idx):\n",
        "        np.random.seed(self.seed)\n",
        "        # Initialize latent factors\n",
        "        self.P = np.random.normal(scale=0.1, size=(n_users, self.n_factors))\n",
        "        self.Q = np.random.normal(scale=0.1, size=(n_items, self.n_factors))\n",
        "        # biases\n",
        "        self.bu = np.zeros(n_users)\n",
        "        self.bi = np.zeros(n_items)\n",
        "        self.mu = train_df['rating'].mean()\n",
        "        # prepare training triples\n",
        "        self.train_triples = [\n",
        "            (user_to_idx[u], item_to_idx[i], float(r))\n",
        "            for u,i,r in zip(train_df['user_id'], train_df['item_id'], train_df['rating'])\n",
        "        ]\n",
        "        # train\n",
        "        for epoch in range(self.n_epochs):\n",
        "            random.shuffle(self.train_triples)\n",
        "            total_loss = 0.0\n",
        "            for (u, i, r) in self.train_triples:\n",
        "                pred = self.predict_single(u, i, clip=False)\n",
        "                err = (r - pred)\n",
        "                total_loss += err**2\n",
        "                # update biases\n",
        "                self.bu[u] += self.lr * (err - self.reg * self.bu[u])\n",
        "                self.bi[i] += self.lr * (err - self.reg * self.bi[i])\n",
        "                # update latent factors\n",
        "                P_u = self.P[u].copy()\n",
        "                Q_i = self.Q[i].copy()\n",
        "                self.P[u] += self.lr * (err * Q_i - self.reg * P_u)\n",
        "                self.Q[i] += self.lr * (err * P_u - self.reg * Q_i)\n",
        "            rmse_epoch = sqrt(total_loss / len(self.train_triples))\n",
        "            if self.verbose:\n",
        "                print(f\"Epoch {epoch+1}/{self.n_epochs} RMSE(train)={rmse_epoch:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict_single(self, u_idx, i_idx, clip=True):\n",
        "        pred = self.mu + self.bu[u_idx] + self.bi[i_idx] + self.P[u_idx].dot(self.Q[i_idx])\n",
        "        if clip:\n",
        "            return min(5.0, max(1.0, pred))\n",
        "        return pred\n",
        "\n",
        "    def predict(self, user_idx, item_idx):\n",
        "        return self.predict_single(user_idx, item_idx)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 6: Evaluation Metrics\n",
        "# ------------------------------\n",
        "def rmse_on_df(model_predict_fn, test_df, user_to_idx, item_to_idx):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for _, row in test_df.iterrows():\n",
        "        u, i, r = row['user_id'], row['item_id'], row['rating']\n",
        "        if u not in user_to_idx or i not in item_to_idx:\n",
        "            continue\n",
        "        uidx = user_to_idx[u]\n",
        "        iidx = item_to_idx[i]\n",
        "        y_true.append(r)\n",
        "        y_pred.append(model_predict_fn(uidx, iidx))\n",
        "    return sqrt(mean_squared_error(y_true, y_pred)) if y_true else None\n",
        "\n",
        "def precision_recall_at_k(recommend_fn, train_df, test_df, user_to_idx, item_to_idx, K=10):\n",
        "    \"\"\"\n",
        "    For each user in test_df (assumes leave-one-out where each user has one test item),\n",
        "    generate top-K recommendations excluding items in train for that user.\n",
        "    \"\"\"\n",
        "    # build train lookup per user\n",
        "    train_by_user = train_df.groupby('user_id')['item_id'].apply(set).to_dict()\n",
        "    test_by_user = test_df.groupby('user_id')['item_id'].apply(set).to_dict()\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    users_evaluated = 0\n",
        "\n",
        "    all_items = set(item_to_idx.keys())\n",
        "\n",
        "    for user in test_by_user:\n",
        "        if user not in user_to_idx:\n",
        "            continue\n",
        "        uidx = user_to_idx[user]\n",
        "        train_items = train_by_user.get(user, set())\n",
        "        candidate_items = sorted(all_items - train_items)  # item ids (original)\n",
        "        # map candidates to indices\n",
        "        candidate_idx = [item_to_idx[i] for i in candidate_items if i in item_to_idx]\n",
        "        # score all candidates\n",
        "        scores = [(iidx, recommend_fn(uidx, iidx)) for iidx in candidate_idx]\n",
        "        # top-K by score\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        topk = [iidx for iidx, _ in scores[:K]]\n",
        "        # map topk indices back to original item ids\n",
        "        idx_to_item = {v:k for k,v in item_to_idx.items()}\n",
        "        topk_items = set(idx_to_item[i] for i in topk)\n",
        "        test_items = test_by_user[user]\n",
        "        hit_count = len(topk_items & test_items)\n",
        "        precisions.append(hit_count / K)\n",
        "        recalls.append(hit_count / len(test_items))\n",
        "        users_evaluated += 1\n",
        "\n",
        "    return (np.mean(precisions), np.mean(recalls), users_evaluated)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 7: Example pipeline (runs everything)\n",
        "# ------------------------------\n",
        "def run_example(download=True):\n",
        "    # 1) download\n",
        "    if download:\n",
        "        try:\n",
        "            download_movielens_100k()\n",
        "        except Exception as e:\n",
        "            print(\"Download failed:\", e)\n",
        "            print(\"If offline, ensure data/ml-100k is present.\")\n",
        "    # 2) load\n",
        "    df = load_movielens_100k(path='data/ml-100k/u.data')\n",
        "    print(\"Loaded ratings:\", df.shape)\n",
        "    # 3) split\n",
        "    train_df, test_df = leave_one_out_split(df, seed=42)\n",
        "    print(\"Train size:\", len(train_df), \"Test size:\", len(test_df))\n",
        "    # 4) mappings\n",
        "    user_to_idx, item_to_idx, idx_to_user, idx_to_item = create_mappings(pd.concat([train_df, test_df]))\n",
        "    n_users = len(user_to_idx); n_items = len(item_to_idx)\n",
        "    print(\"n_users, n_items:\", n_users, n_items)\n",
        "    # 5) build matrix (train only)\n",
        "    train_mat = build_user_item_matrix(train_df, user_to_idx, item_to_idx, shape=(n_users, n_items))\n",
        "    user_means = np.zeros(n_users)\n",
        "    for u, group in train_df.groupby('user_id'):\n",
        "        user_means[user_to_idx[u]] = group['rating'].mean()\n",
        "    # 6) compute similarities (item-based recommended for speed)\n",
        "    item_sim = compute_item_similarity(train_mat)\n",
        "    user_sim = compute_user_similarity(train_mat)  # optional\n",
        "\n",
        "    # 7) define prediction wrappers\n",
        "    def item_predict_fn(u_idx, i_idx):\n",
        "        return predict_item_based(u_idx, i_idx, train_mat, item_sim, k=20)\n",
        "\n",
        "    def user_predict_fn(u_idx, i_idx):\n",
        "        return predict_user_based(u_idx, i_idx, train_mat, user_sim, k=20, user_means=user_means)\n",
        "\n",
        "    # 8) Evaluate CF methods by RMSE\n",
        "    rmse_item = rmse_on_df(item_predict_fn, test_df, user_to_idx, item_to_idx)\n",
        "    rmse_user = rmse_on_df(user_predict_fn, test_df, user_to_idx, item_to_idx)\n",
        "    print(f\"RMSE Item-based CF: {rmse_item:.4f}\")\n",
        "    print(f\"RMSE User-based CF: {rmse_user:.4f}\")\n",
        "\n",
        "    # 9) Precision@K (leave-one-out) for item-based\n",
        "    prec_item, rec_item, ucount = precision_recall_at_k(item_predict_fn, train_df, test_df, user_to_idx, item_to_idx, K=10)\n",
        "    print(f\"Item-CF Precision@10: {prec_item:.4f} Recall@10: {rec_item:.4f} (on {ucount} users)\")\n",
        "\n",
        "    # 10) Train FunkSVD (matrix factorization) and evaluate\n",
        "    mf = FunkSVD(n_factors=30, lr=0.005, reg=0.02, n_epochs=15, verbose=True)\n",
        "    mf.fit(train_df, n_users, n_items, user_to_idx, item_to_idx)\n",
        "    def mf_predict_fn(u_idx, i_idx):\n",
        "        return mf.predict(u_idx, i_idx)\n",
        "\n",
        "    rmse_mf = rmse_on_df(mf_predict_fn, test_df, user_to_idx, item_to_idx)\n",
        "    print(f\"RMSE Matrix Factorization (FunkSVD): {rmse_mf:.4f}\")\n",
        "\n",
        "    prec_mf, rec_mf, _ = precision_recall_at_k(mf_predict_fn, train_df, test_df, user_to_idx, item_to_idx, K=10)\n",
        "    print(f\"MF Precision@10: {prec_mf:.4f} Recall@10: {rec_mf:.4f}\")\n",
        "\n",
        "    # 11) quick example: recommend top-10 for a given user\n",
        "    example_user_orig_id = list(user_to_idx.keys())[0]\n",
        "    uidx = user_to_idx[example_user_orig_id]\n",
        "    candidate_items_idx = list(range(n_items))\n",
        "    scores = [(iidx, mf_predict_fn(uidx, iidx)) for iidx in candidate_items_idx]\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    top10 = scores[:10]\n",
        "    print(\"Top-10 recommendations (item_id,score):\")\n",
        "    for iidx, sc in top10:\n",
        "        print(idx_to_item[iidx], f\"{sc:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_example(download=True)\n"
      ]
    }
  ]
}